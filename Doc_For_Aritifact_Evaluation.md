

- [Setup \&\& Configure](#setup--configure)
- [Evaluation](#evaluation)
  - [0. Minimal Running Example](#0-minimal-running-example)
  - [1. Specifcation Generation (Section VII.A).](#1-specifcation-generation-section-viia)
    - [1.0 Minimal Running](#10-minimal-running)
    - [1.1 Normal Test For Specfication Generation](#11-normal-test-for-specfication-generation)
  - [2. Bug Detection (Section VII.B).](#2-bug-detection-section-viib)
    - [2.0 Minimal Running For Bug Detection](#20-minimal-running-for-bug-detection)
    - [2.1 NormalTest For Bug Detection](#21-normaltest-for-bug-detection)
  - [3. Utilizebility of API Aritifacts (Section VII.D).](#3-utilizebility-of-api-aritifacts-section-viid)
  - [4. Compared with related work. (Section VII C.)](#4-compared-with-related-work-section-vii-c)


## Setup && Configure
Please refer to [INSTALL](./INSTALL) for installation and preparation.


In our evaluation, we used a 64-bit Ubuntu 22.04 system with 503GB of memory, powered by an Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz with 79 processors.
APISpecGen works on standard machines and does not require significant memory overhead.

We recommend using multithreading to speed up the evaluation process. By default, 48 threads are used, but without multithreading, the process may take significantly longer. You can adjust the num_threads setting in the config.cfg file according to your needs. 

The execution time for the evaluation depends on the performance of the machine used. To avoid unexpectedly long evaluation times, you can limit the max_depth (default is 10) in the script/1.specification_extraction.sh to a smaller value (such as 1 or 2) during the specification generation phase. This will allow you to test the evaluation and estimate the time required.


## Evaluation
In the evaluation, we provided a minimal running example for quick test. Then we provide the evalution that shown in the paper.

### 0. Minimal Running Example
This is related to the working example that displayed in the paper (as shown below). 

- Please follow the [Specification Generation for working example](#10-minimal-running), which generates the shown specifcation (`nfc_get_device, nfc_put_device, `retval`).

- Follow the [Bug detection for working example](#20-minimal-running-for-bug-detection), which detects the `nfc_genl_vendor_cmd` function as buggy.




<img src=./assets/APISpecGen-working-example.png#pic_center width=60% />




### 1. Specifcation Generation (Section VII.A).

#### 1.0 Minimal Running 
- **[Intro]** Perform specfication progation analysis for a seed with iteration set to 2.
- **[Run]** `./script/0.quick_spec_generate.sh `
- **[Results]** 
The generated specification saved to `GeneratedSpecs/linux_get_device_1_generated_specs.json`
The reference data is `ReferenceData/LinuxKernel/linux_get_device_1_generated_specs.json`
Below is a generated specifcation example
```shell
{
  "API": "nfc_get_device", // the inferred API
  "SecOp": "nfc_put_device",// the inferred post-operation for the API     
  "usageCount": 21, // the usage times that follows the specifcation in the programs
  "depth": 2, // the propogation depth 
  "API_path": "get_device->class_find_device->nfc_get_device", // The API's propogation chain 
  "var_path": "arg->retval->retval" // the critical variable's propogation chain 
},
```
- **[Execution time]** 
```
real    0m23.331s
user    12m19.762s
sys     12m35.620s
```

#### 1.1 Normal Test For Specfication Generation
- **[Intro]** Generate specifcations use the given six seed specifcations.  
- **[Run]** `./script/1.specification_extraction.sh`   
- **[Result]** The specifications generated by different Seed API is saved to `linux_{seedAPI}_10_generated_specs.json` represtively.

Run `summarize_spec_results.py` for statical analysis.
Example output is shown below (the numbers may be excatly the same, but they confirm our main claim):
```shell
API Item Count Summary:
              API  Item Count
       get_device         154
          kstrdup          22
device_initialize          59
          kmalloc        1408
   try_module_get          79
          ERR_PTR        4328
```

- **[Execution Time]**
```
real    31m12.280s
user    1143m48.649s
sys     1076m31.634s
```
### 2. Bug Detection (Section VII.B).

#### 2.0 Minimal Running For Bug Detection
- [Run] `./script/0.1.quick_bug_detection_for_test.sh` 
- **[Intro]** Perform quick test for bug detection using one generated specification [obtained from specifcation generation for working example](#10-minimal-running). Use the specifcation generated from step (nfc_get_device, nfc_put_device,retval) to detect bugs. 
- **[Results]** 
The script prints out the detected potential bugs. This includes the buggy function `nfc_genl_vendor_cmd` displayed in the paper.
The output example: 
```shell
[checked report] nfc_genl_se_io may lack post-operation (['nfc_put_device']) for nfc_get_device
[checked report] nfc_genl_stop_poll may lack post-operation (['nfc_put_device']) for nfc_get_device
[checked report] rawsock_connect may lack post-operation (['nfc_put_device']) for nfc_get_device
[checked report] nfc_genl_vendor_cmd may lack post-operation (['nfc_put_device']) for nfc_get_device
```
Manual confirm that the below four reports are true bugs (tested on Linux kernel v5.6).
- **[Execution Time]**
```shell
real    0m1.575s
user    0m6.052s
sys     0m8.942s
```
#### 2.1 NormalTest For Bug Detection
- **[Intro]** Use generated specifcations to detect new bugs in the Linux kernel. To facilates evaluation, we provided a set of specifcations that related to detected bugs for valiatation. All the used Specification are previously generated by APISpecGen. 

- **[Run]** `./script/2.bug_detection.sh`

- **[Results]** Using the specifcations, APISpecGen detects hundreads of new bugs in the Linux kernel. The bugs reports is saved to file `{WORKDIR}/BugDetection/bug_report.csv`. The bug report example:
```csv
repo_name,buggy_func,main_api,sec_op,var_type,var,scope,violated_path_num
kernel,nfc_genl_vendor_cmd,nfc_get_device,nfc_put_device,retval,dev,Local,6
```
- **[Note] ** When evaluating on different versions of the Linux kernel, the bug reports may vary. Additionally, a small amount of manual effort is required to validate whether the reported bugs are true. Nevertheless, the results confirm our main claim that the generated specifications can detect numerous new bugs.
- 

### 3. Utilizebility of API Aritifacts (Section VII.D).
- **[Intro]** Use the generated specifications to evaluate the usability of API artifacts (including API documentation, API names, and API usage) for specification extraction. We use paired specifications for evaluation because these artifacts are commonly used to extract such specifications.
- **[Run]** `./script/3.API_aritifact_analysis.sh`
- **[Note]** In the evaluation, we provide the previously generated specifications for analysis. Alternatively, you can specify the spec_file in APIAritifactEval/APIAritifactEval.py. The results should be similar.
- **[Results]**  The analysis data reveals that API artifacts have significant limitations in specification extraction. 
An example output is shown below:
```shell
[API Name Analysis]:15.96% APIs do not contain the informative subwords (verbs) (332 out of 2080).
[API Usage Analysis]: 93.80% API Pairs occur less than 10 times, 88.07% API Pairs occur less than 5 times.
[API Doc Analysis]: 95.00% of Specs not mentioned in Doc For All APIs 1976/2080
[API Doc Analysis]: 76.31% of Specs not mentioned in Doc for those API having docs: 335/439
```

### 4. Compared with related work. (Section VII C.)
In the paper, we compare APISpecGen with SinkFiner, APHP, Advance, and IPPO.

In the artifacts, we provide the specifications collected from SinkFiner and APHP, located in ComparedWithRelatedWork/RelatedWorkData, which are used to conclude the comparative results.

For Advance, we reference the API doc for specfication extraction to obtain the results.

For IPPO, we directly run it to conclude the results.


